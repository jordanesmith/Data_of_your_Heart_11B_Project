{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057e4e61-945f-4180-ac54-3508d5bcdeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c649e9-8d20-489f-9cfa-7efc238aba4c",
   "metadata": {},
   "source": [
    "### Performance metrik function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47802ef1-151c-4fcd-96c8-640a056d62dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metriks for 'N' diagnosis unsuccessful due to divide by 0 error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9473684210526315, 1.0, 0.9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ = np.array([[f\"d{i}\", \"A\"] for i in range(10)])\n",
    "lab_ = np.vstack((np.array([[f\"d{i}\", \"A\"] for i in range(9)]), np.array([\"d9\", \"N\"])))\n",
    "\n",
    "def find_f1_score(predictions, labels):\n",
    "    \"\"\"\n",
    "    Calculates performance metriks for 'A' or 'N' categorisation from provided\n",
    "    predictions and labels numpy arrays.\n",
    "    \n",
    "    returns:\n",
    "        (f1_score: float, precision: float, recall: float)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert predictions.shape[0] == predictions.shape[0]\n",
    "    \n",
    "    N = predictions.shape[0]\n",
    "    \n",
    "    arr_predictions_labels_only = np.zeros((N,2), dtype=str)\n",
    "    counter = 0\n",
    "    for filename, pred_ in predictions:\n",
    "        label_ = labels[labels[:,0] == filename][0][1]\n",
    "        arr_predictions_labels_only[counter, 0] = label_ \n",
    "        arr_predictions_labels_only[counter, 1] = pred_\n",
    "        counter += 1\n",
    "    \n",
    "    tp = 0 # no. true positives\n",
    "    fp = 0 # no. false positives\n",
    "    fn = 0 # no. false negatives\n",
    "    tn = 0 # no. true negatives\n",
    "    \n",
    "    for prediction_label_pair in arr_predictions_labels_only:\n",
    "        pred, lab = prediction_label_pair\n",
    "        if pred == \"N\" and lab == \"N\": tn += 1\n",
    "        elif pred == \"A\" and lab == \"A\": tp += 1\n",
    "        elif pred == \"N\" and lab == \"A\": fn += 1\n",
    "        elif pred == \"A\" and lab == \"N\": fp += 1\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy metriks for AF diagnosis\n",
    "    A_diagnosis_metriks_calculated = False\n",
    "    try:\n",
    "        precision_A = tp / (tp + fp)\n",
    "        recall_A = tp / (tp + fn)\n",
    "        f1_score_A = 2 / ( recall_A**(-1) + precision_A**(-1) )\n",
    "        A_diagnosis_metriks_calculated = True\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Performance metriks for 'A' diagnosis unsuccessful due to divide by 0 error\")\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy metriks for Not AF diagnosis\n",
    "    N_diagnosis_metriks_calculated = False\n",
    "    try:\n",
    "        precision_N = tn / (tn + fn)\n",
    "        recall_N = tn / (tn + fp)\n",
    "        f1_score_N = 2 / ( recall_N**(-1) + precision_N**(-1) )\n",
    "        N_diagnosis_metriks_calculated = True\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Performance metriks for 'N' diagnosis unsuccessful due to divide by 0 error\")\n",
    "        \n",
    "    if A_diagnosis_metriks_calculated and N_diagnosis_metriks_calculated:\n",
    "        return [(f1_score_A, precision_A, recall_A), ((f1_score_N, precision_N, recall_N))]\n",
    "    elif A_diagnosis_metriks_calculated and not N_diagnosis_metriks_calculated:\n",
    "        return f1_score_A, precision_A, recall_A\n",
    "    elif not A_diagnosis_metriks_calculated and N_diagnosis_metriks_calculated:\n",
    "        return f1_score_N, precision_N, recall_N\n",
    "        \n",
    "find_f1_score(pred_,lab_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8249795b-3b54-41aa-8b4c-96508fe58b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24188611-0ff9-4f82-a38a-350a1463151b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['d0', 'A'],\n",
       "        ['d1', 'A'],\n",
       "        ['d2', 'A'],\n",
       "        ['d3', 'A'],\n",
       "        ['d4', 'A'],\n",
       "        ['d5', 'A'],\n",
       "        ['d6', 'A'],\n",
       "        ['d7', 'A'],\n",
       "        ['d8', 'A'],\n",
       "        ['d9', 'A']], dtype='<U2'),\n",
       " array([['d0', 'A'],\n",
       "        ['d1', 'A'],\n",
       "        ['d2', 'A'],\n",
       "        ['d3', 'A'],\n",
       "        ['d4', 'A'],\n",
       "        ['d5', 'A'],\n",
       "        ['d6', 'A'],\n",
       "        ['d7', 'A'],\n",
       "        ['d8', 'A'],\n",
       "        ['d9', 'N']], dtype='<U2'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc8529-e65a-45e4-b9d9-e30f09fb7ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
